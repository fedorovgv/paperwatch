# FSDP

---

![Снимок экрана 2024-05-19 в 14.07.01.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-19_%25D0%25B2_14.07.01.png)

---

# Fully Sharded Data Parallel

---

Arxiv [[link](https://arxiv.org/abs/2304.11277)]

Getting Started with FDSP [[link](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)]

Docs [[link](https://pytorch.org/docs/stable/fsdp.html)]

FSDP at HuggingFase [[link](https://huggingface.co/docs/accelerate/usage_guides/fsdp)]

---

# ABSTRACT

Модели становятся все больше по размеру и все меньше людей и компаний могут их обучать.

В статье представлен инструмент для обучения больших моделей - FSDP, который вписан в экосистему torch и который эффективно использует фичи последнего, такие как планировщик памяти CUDA (эффективно переиспользуем аллоцированную память на GPU), абстракцию тензора, планировщик задач (перекрываем вычисления с движениями данных).

Кроме того FSDP использует большое количество оптимизаций (по сути FSDP это ZeRo stage 3, написанный командой торча, с кучей оптимизаций и с эффективным использованием базовой инфраструктуры торча).

FSDP способен показывать сравнимый с DPP результат при линейном масштабировании мощности в TFLOPS для гораздо больших по размеру моделей.

# INTRODUCTION

Еще раз о подходах для обучения больших моделей:

![Untitled](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/Untitled.png)

Основные:

- Pipeline Parallelism
- Tensor Parallelism
- Zero-Redundancy Parallelism

![Untitled](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/Untitled%201.png)

Общие минусы таких подходов:

- тесная интеграция с конкретной архитектурой модели
- такие системы построены на быстро развивающихся внешних базовых фреймворках, поэтому они становятся чувствительными к изменениям последних

FSDP:

- мотивирован ZeroRedundancy Stage 3, однако переделан с использованием инфраструктуры torch
- **суть**:
    - есть большая модель
    - делим модель на части - юниты, которые состоят из слоев целиков
    - разворачиваем юниты в 1d тензор
    - паддим этот тензор так, чтобы его длина делилась на количество карт
    - нарезаем этот тензор на равные части - шарды и раскладываем их по картам
    - затем как в ZeRo, собираем шарды для forward / backward и удаляем после вычислений
- собирать шарды нужно только из одного юнита, что значительно снижает пиковое использование памяти

![Untitled](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/Untitled%202.png)

FSDP challenges:

- **User Experience**: хотелось бы такой же удобный интерфейс, как у DPP с единственным отличием, что иногда мы не можем инициализировать все веса на девайсе сразу
- **Hardware Heterogeneity**: хотелось бы эффективно работать как внутри ноды, так и между ними
- **Resource Utilization**: снизить время на non-computation операции
- **Memory Planning**: pytorch делает аллокацию памяти на GPU эффективно за счет кэширования, боремся с дефрагменацией

Методы:

- ребята написали метод отложенной инициализации, который помогает инициализировать модель, которая не влезает в память девайса
- FSDP позволяет настраивать шардирование для преодоления гетерогенности девайсов, для этого есть встроенные политики разбиений
- Коммуникация шардов может быть ботелнеком для вычислений, поэтому FSDP пытается бороться с этим за счет их перекрытия
- FSDP оптимизирует использование памяти на девайсе регулируя скорость выделения памяти под коллективные операции и вычисления

# BACKGROUND

Вспомним базовый примитив библиотеки, такой как `torch.nn.Module`, который состоит из `torch.tensor` (например, `nn.Linear` состоит из `weights` и `bias`). Последний связан с хранилищем на девайсе, в случае простых операций, это хранилище может не изменяться. Преобразование тензора на входе в `torch.nn.Module` описывается функцией `forward` .

Растут как модели, так и входные тензоры, поэтому необходимо построить инструмент для масштабирования обучения.

### Model replication

DDP позволяет обрабатывать больший объем данных за счет репликации модели и разбиения данных по девайсам. Во время backward pass мы обновляем градиенты с помощью All Reduce, обспечивая согласованность весов на каждом девайсе. Для ускорения, мы совмещаем обратный проход с коммуникацией градиентов. Главное ограничение подхода в том, что мы должны хранить веса, градиенты, стейты оптимайзера на девайсах. Поэтому, мы скорее всего не сможем тренировать модель в 1B на A100 40Gb.

### Model Partitioning

Сюда входит Pipeline parallelism, Tensor RPC. Как уже говорилось выше, у этих методов главные проблемы в том, что модель будет задействована частями или в том, что нам придется переписывать архитектурно обучение и модели.

### Model sharding

В данном подходе мы разбиваем модель на части - шарды и раскладываем по девайсам. Это уменьшает использование памяти. Но чтобы шардирование было корректным, должно выполняться одно из условий:

- **не передаем шарды, передаем активации** - может быть супер экономно по памяти, но супер затратно по времени, поскольку карты будут простаивать ожидая коммуникаций;
- **передаем шарды, не передаем активации** - поскольку материализация шардов не зависит от предыдущих активаций, то передача может быть наложена на вычисление активаций.

FSDP использует второе условие по результатам экспериментов.

# SYSTEM DESIGN

## Intro

FSDP разбивает каждый блок модели на юниты и шарит каждый юнит отдельно.

![Снимок экрана 2024-05-19 в 16.11.23.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-19_%25D0%25B2_16.11.23.png)

На картинке выше FSDP обернула модель из шести слоев в три юнита и пошарила веса на две карты.

Показан forward и backward для первого юнита. Перед форвардом мы делаем unshared юнитов через AllGather, делаем локально форвард, удаляем те части юнитов которые пришли с других карт.

Для backward аналогично, только после удаления частей юнита, мы должны через ReduceScatter пошарить градиенты на другие карты, после чего у нас останется только та часть градиентов, которая была изначально.

![Untitled](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/Untitled%203.png)

Состояния оптимайзера остаются пошаренными всегда.

Пиковую память на девайсе можно оценить как O(пошаренная память модели + наибольший материализованный юнит).

Разбиение на юниты может контролироваться пользователем через функцию. 

## Model Initialization

Тут есть куча штук, основная задача которых, побороть случай, когда модель при инициализации не влезает в память девайса (редкий случай). А также снять с пользователя ответственность, за шардирование модели, потому что часто пользователь использует штуки из third party библиотек и это будет сложно переписывать и разбивать на шарды. Не хочу останавливаться на этом очень надолго и подробно. 

Суть в том, что ребята придумали отложенную инициализацию (deferred initialization) модели и написали под это торчевую либу ([deferred init API](https://pytorch.org/torchdistx/latest/deferred_init.html), [fake tensor API](https://pytorch.org/torchdistx/latest/fake_tensor.html), [guide](https://pytorch.org/torchdistx/latest/deferred_init.html)).

Automated parallelism libraries (e.g. FSDP, DeepSpeed) either completely ignore this problem, meaning they expect the model to fit on a single machine, or they have some rudimentary workarounds to partially overcome it. 

Суть отложенной инициализации в том, чтобы обходить модули модели последовательно и записывать функции, которые мы используем для инициализации весов (например, где-то nn.Linear мы будем инициализировать функцией nn.init.xavier_uniform_). После такой инициализации без инициализации, перед началом обучения FSDP также последовательно будет обходить модель, подгружать юниты, выполнять соответствующие функции и раскидывать веса по девайсам.

## Sharding Strategies

### Intro

[docs](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.ShardingStrategy)

FSDP поддерживает различные типы разбиения от полной дубликации модели, по сути DDP, до полного шардирования, когда на каждом девайсе хранится равная доля весов.

Введем буквы: 

**F** - количество девайсов, на которое шарим модель

**N** - количество девайсов всего

**W** - количество весов в модели

|  | на сколько девайсов шарим |  сколько весов на каждом девайсе | комментарий |
| --- | --- | --- | --- |
| NO SHARD |  1 |  W | ровно DDP, удобно на инференсе |
| SHARD_GRAD_OP |  1 | W | почти ZeRo stage 2, только веса все равно шарят, материализуют перед forward и удаляют после backward, то есть веса пошаренные вне стадии вычислений |
| FULL_SHARD | N |  W / N | ZeRo stage 3, веса и градиенты шарят, материализуют перед forward/backward и удаляют после |
| HYBRID_SHARD | N | W / N | FULL_SHARD внутри ноды, дубликация по нодам, таким образом мы уменьшаем количество дорогих частотных коммуникаций между нодами |
| _HYBRID_SHARD_ZERO2 | N  | W / N | SHARD_GRAD_OP внутри ноды, дубликация по нодам; Аналогично HYBRID_SHARD, только без удаления материализованных весов после forward, как в SHARD_GRAD_OP; |

### Full Sharding

При полном шардировании у нас увеличивается **объем** коммуникаций в 1.5 раза, как это было показано в ZeRo stage 3. Но FSDP старается сделать коллективные коммуникации - AllGather, ReduceScatter, эффективными. 

![Снимок экрана 2024-05-19 в 17.51.37.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-19_%25D0%25B2_17.51.37.png)

Как повысить эффективность коммуникаций?

- **Выполнять коллективные коммуникации для тензоров равной длины**
    - За коллективные коммуникации отвечает NVIDIA NCCL. В ней реализация NCCL AllGather требует равных длин тензоров, потому что складывает результат в заранее приготовленный тензор.
    - В торче за коммуникацию отвечает Pytorch Process Group, который суть обертка над NCCL и может выполнять коммуникации для тензоров с разными длинами, но по сути складывает эти тензоры в один и делает вызов NCCL AllGather, что не эффективно.
    - На рисунке 2а. AllGather Base - NCCL Allgather, AllGather - обертка Pytorch Process Group.
- **Меньше коммуникаций с большим объемом**
    - Для фиксированного объема коммуникаций лучше пересылать больше данных за раз, чем много раз пересылать небольшие объемы, что видим на 2b.

Мы должны делать большое количество коммуникаций для **юнитов** во время обучения. Из наших наблюдений выше, рождается `FlatParameter` - 1d тензор, в который мы запаковываем юнит.

![Снимок экрана 2024-05-19 в 18.08.16.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-19_%25D0%25B2_18.08.16.png)

Пусть у нас единственный юнит, в который мы обернули `nn.Linear(4, 3)`:

- вытягиваем веса в 1d тензор - FlatParameter
- паддим тензор так, чтобы он делился на количество девайсов F
- делим FlatParameter на чанки одинаковой длины и раскладываем по девайсам

В пике модель будет занимать память пропорцинальную размеру модели и самому большому юниту.

$$
\Psi - \text{количество параметров} \\ \text{N - количество юнитов} \\ \sum_{i=1}^{N} \psi_i = \Psi \\ \text{Пиковая память - O} \left( \frac{\Psi}{F} + \max_{i \in [1, .., N]} \psi_i \right)
$$

Отсюда возникает trade off между памятью и коммуникациями - можно обернуть много модулей и уменьшить второе слагаемое, но при этом возрастет количество коммуникаций и наоборот.

Эту проблему может решать пользователь указывая политику для разбиения модели.

### Hybrid Sharding

Ситуация, когда F между 1 и суммарным количеством девайсов называется гибридным шардированием. Напомню, что у нас много дорогих коммуникаций весов, но нам бы хотелось учить модели, например, в мультинодовом режиме. Поэтому для D нод по N девайсов мы можем сделать полное шардирование на N частей внутри каждой ноды и полное дублицирование по нодам. 

![Снимок экрана 2024-05-19 в 19.17.19.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-19_%25D0%25B2_19.17.19.png)

Для обновления градиентов нам все еще нужно сделать ReduceScatter внутри каждой из шардированных групп, затем по реплицирванным группам. 

Такой сетап позволяет выиграть по объему (не по скорости) коммуникаций у DPP.

Подходим для medium моделей, которые достаточно большие, чтобы обучать их на одной GPU, но не достаточно, чтобы делать полное шардирование

### Autograd

Важное замечание в том, что FlatParameter написан таким образом, что внутри он хранит как веса, так и соответствующие градиенты модели. Это связано с тем, что ребята стараются заюзать autograd, а не переписывать его для себя. Это облегчает жизнь разработчикам торча, но для нас делает не очень эффективным, например, дообучение LLM c LoRa.

![Untitled](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/Untitled%204.png)

Потому что мы не можем пошарить только веса и не хранить для них градиенты.

## Communication Optimizations

Основные оптимизации тут, это backward, forward prefetching и gradient accumulation.

### **Overlapping Communication and Computation**

Идея простая, поскольку у нас много дорогих коммуникаций, которые при этом еще и ботеллнек для вычислений, то мы бы хотели пересекать некоторые коммуникации с вычислениями.

В статье подробно написано как они **немного ломают** ProcessGroupNCCL - NCCL бэкенд для коммуникаций, а точнее просто добавляют второй стрим, который отвечает за коммуникации, чтобы можно было делать пересечение коммуникаций и вычислений. Причина в том, что ProcessGroup писали во времена DDP и он может делать вычисления после вычислений (AllGather в DDP после backward, а нам нужно делать AllGather до вычислений). 

![Untitled](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/Untitled%205.png)

- тык
    
    ![Снимок экрана 2024-05-19 в 16.11.23.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-19_%25D0%25B2_16.11.23.png)
    

Пример судя по всему для модели для модели с картинки выше.

![Снимок экрана 2024-05-20 в 15.04.01.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-20_%25D0%25B2_15.04.01.png)

### **Backward Prefetching**

[docs](https://pytorch.org/docs/stable/fsdp.html#torch.distributed.fsdp.BackwardPrefetch)

В FSDP мы должны собрать веса юнита для backward pass через AllGather, посчитать градиенты, затем сделать ReduceScatter, чтобы разложить части градиентов, после чего вызвать AllGather для следующего юнита.

![Снимок экрана 2024-05-19 в 16.11.23.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-05-19_%25D0%25B2_16.11.23.png)

Как на картинке, AllGather для Unit1, затем ReduceScatter для Unit1, затем снова AllGather для Unit0 и так далее. 

Проблема в том, что ProcessGroupNCCL будет делать это последовательно и это ботеллнек для вычислений. Отсюда ребята делают backward prefetching, мы делаем AllGather для Unit0 до того, как сделать ReduceScatter для Unit0.

### Forward **Prefetching**

В редких случаях с медленным CPU мы можем попросить FSDP сделать AllGather для следующего после текущего юнита AllGather. По дефолту это не используется.

### Gradient Accumulation

Тут вроде все просто, мы можем выполнять коммуникации до степа оптимайзера, а можем нет. Во втором случае больше память, но нет расходов на коммуникацию.

## Memory Management

### How Does PyTorch Caching Allocator Affect Memory

[docs](https://zdevito.github.io/2022/08/04/cuda-caching-allocator.html)

Под капотом `torch` использует `CUDA caching allocator` для эффективного управления памятью. Он нужен потому что, во время обучения мы постоянно приносим и уносим с памяти девайса тензоры. Чтобы выделить и освободить память на GPU на нужно вызвать `cudaMalloc` and `cudaFree`. Многие последовательные вызовы крайне не желательны, поскольку они требуют синхронизации стримов на девайсе, чтобы убедиться в том, что память которую они собираются выделить или удалить не нужна для других стримов.

Как нам помогает аллокатор?

При удалении тензора, аллокатор не возвращает, память которую он занимал, а кладет ее в пул. Такую память может затем переиспользовать следующий тензор, тем самым мы избегаем дорогостоящих манипуляций.

Мы все знаем, что в торче есть функции `memory_allocated()` и ****`memory_reserved()` . Первая функция показывает память, которую занимают настощие тензоры, вторая функция показывает память, которой оперирует аллокатор.

### Rate Limiter

На самом деле, чтобы перекрывать операции с вычислениями из предыдущего пункта ребята считерили и создали для этого отдельный CUDA стрим, который выполняет коллективные операции. Отсюда сразу могут возникать куча проблем, потому что аллокатор работает для каждого стрима отдельно. Так, теперь один из аллокаторов может забирать память девайса, например, под коллективные операции и это может привести к тому, что нам не хватит памяти. Чтобы пофиксить это, ребята написали rate limiter, который ограничивает количество запросов на выделение памяти.

# IMPLEMENTATION

## Initialization

Бывают редкие случаи, когда отложенная инициализация не возможна, например, когда инициализация одного параметра зависит от инициализации другого параметра, который может быть уже пошарен к моменту инициализации последнего.

В таких случаях FSDP умеет:

- **Инициализировать модель на GPU**, а затем обернуть ее в FSDP, то есть обернуть в юниты и пошарить их.
- **Инициализировать модель на CPU**, в данном случае FSDP оборачивает модель юнит за юнитом, перекладывает их на GPU и сразу делать шардирование.

Конечно, оба метода несут накладные расходы. Первый не подходит для очень больших моделей, второй имеет оверхед на миграцию весов с CPU на GPU.

Важно, что пользователи могут указать функцию политики обертки модуля и его сабмодулей - `auto_wrap_policy`. 

## FlatParameter

`FlatParameter` наследуется от `nn.Parameter` и по поведению похож на него. Также с каждым `FlatParameter` ассоциирован `FlatParamHandle`, который отвечает за работу FSDP с `FlatParameter`. 

Поскольку `FlatParameter` хранит юнит целиком, то **нам важно, чтобы юниты были достаточными по объему для эффективных AllGather и ReduceScatter**. Еще **нам бы хотелось, чтобы юниты были последовательными**.

Если мы используем дефолтную политику обертки юнитов, то FSDP следует правилу, что модуль и его сабмодули, кроме уже обернутых, оборачиваются в один юнит. Как правило, это эффективно.

Еще одна оптимизация состоит в том, что FSDP может динамически изменить структуру юнитов после первого шага обучения, поскольку тогда станет понятен порядок их исполнения. 

## Runtime

Для FSDP крайне важно вовремя вызывать коллективные коммуникации в соответствии с forward и backward модулей и юнитов.

Для этого FSDP переиспользует методы `torch.nn.Module` : `register_forward_pre_hook`, `register_forward_hook` , которые выполняют некоторый хук перед и после форварда соответственно.

Однако изначально, у `torch.nn.Module` нет таких же методов для регистрации хуков на backward pass. Тут авторы используют хуки с других сущностей:

- у tensor есть метод `register_hook` , который позволяет установить хук, который будет исполняться каждый раз, когда у тензора будет посчитать градиент, так мы можем понять, когда закончились вычисления на backward pass;
- у torch.autograd можно выполнить хук, который будет выполнен после окончания подсчета градиентов, таким образом мы будем знать, когда закончились коммуникации градиентов и можно делать степ оптимайзера;
- также у torch.autograd можно выплнить хук, когда градиенты аккумулировались, что так же сигнализирует о возможности коммуникаций;

Тем самым ребята делают тесную интеграцию базовых инструментов torch и FSDP.

## Native Mixed Precision

Базовый torch.amp выполняет операции и хранит при этом fp16/bfp16 и fp32 модели, при этом forward, backward работаю с fp16/bfp16, оптимайзер работает с fp32. FSDP позволяет пользователям настраивать точность для параметров, градиентов и буферов.

FlarParameter из FSDP позволяет динамически менять точность с одной на другую, таким образом нам не нужно хранить две копии модели. Кроме того, коллективные операции можно выполнять в низкой точности, а увеличивать точность после.

# EVALUATION

Тут нам интересно сравнение с ~~ZeRo~~  DDP! 

## Experiment Setup

Основные модели для экспериментов:

- T5-11B
- minGPT-175B
- DHEN (768B Sparse + 550B Dense)

Для DHEN sparse параметры пошарили с помощью tensor parallel. Для валидации эффективности разных стратегий шардирования, префетчинга, использовали T5-611M, T5-2B и T5-11B.

## Model Scale

Сравнение разных подходов:

![Снимок экрана 2024-06-07 в 11.10.29.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_11.10.29.png)

Говорит о том, что FSDP не сильно отстает от DDP и, конечно, работает там, где DDP отваливается по OOM - 11B.

Следующий эксперимент для minGPT-175B показывает, что backward prefetching ускоряет модель на 18%:

![Снимок экрана 2024-06-07 в 11.14.38.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_11.14.38.png)

## Throttle Communications

Напомним, что для пересечения вычислений и коммуникаций, мы сделали отдельный стрим для коммуникаций. Это может быть проблемой, если стрим коммуникаций будет делать много запросов на выделение памяти, что может привести к дефрагментации в стриме вычислений. Из-за этого придумали rate limiter, которые ограничивает количество вызовов на выделение памяти в стриме коммуникаций. В FSDP это флаг `limit_all_gathers` .

Для валидации скорости использовались модели:

- RegNet - 9B, batch size 48 for 2 nodes and 72 for 4 nodes
- T5 - 11B, and batch size 2
- DeepViT - 8B, and batch size 105 for 2 nodes and 120 for 4 nodes

Результаты:

![Снимок экрана 2024-06-07 в 11.24.48.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_11.24.48.png)

Видим, что результаты не консистентны по моделям.

Однако, есть инструмент, который может помочь нам при обучении наших моделей понять, когда точно имеет смысл использовать rate limiter (дефлтное значение - True). В `torch.cuda.memory_stats()`  есть параметр `num_alloc_retries` . Он показывает количество раз, которое торч пытался выделить память перед тем как сдаться и кинуть OOM.

Результаты показывают, что для T5 может быть ускорение в 5 раз, в то время как для DeepViT rate limiter дает 5% замедления. 

## Efficient Training for Large Models

Эффективность для очень больших моделей:

![Снимок экрана 2024-06-07 в 11.36.15.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_11.36.15.png)

QPS - Queries Per Second 

RAF - reshard-after-forward

NRAF - no-reshard-after-forward

B - batch size

Active Memory ⊆ Allocated Memory ⊆ Reserved Memory

- в чем разница?
    - This is queried by `torch.cuda.memory_allocated()`. It gives you the total amount of memory currently used by tensors.
    - PyTorch does not distinguish between active and allocated memory in its current API. Both `torch.cuda.memory_allocated()` and `torch.cuda.memory_reserved()` can give insights into memory usage, but the key difference lies in understanding that all active memory is allocated, and some allocated memory might be cached or reserved for future use.
    - This is queried by `torch.cuda.memory_reserved()`. It indicates the total amount of memory reserved by the CUDA memory manager, including memory that may be currently unused but is set aside for future allocations.

# DISCUSSION

## FSDP Interoperability

Хотелось бы миксовать FSDP и другие техники параллелизма моделей, такие как tensor parallel.

### Pipeline Parallelism

Мы можем с легкостью использовать FSDP в связке с PP, поскольку можем обернуть каждую часть отдельно. 

### Tensor Parallelism

У торча есть либа `torch.distributed.tensor.parallel` , которая может работать в связке с FSDP и обеспечивать 2D parallelism, организовывая девайсы в 2D решетку. По первой размерности FSDP и коммуникация шардами, по второй TP и коммуникация активациями. 

### Limitations

- **Mathematical Equivalence**
    
    Тут прежде всего про корректность степа оптимайзера, поскольку он всегда пошаренный. Если шаг зависит от структуры параметров целиком (например, нужна норма), то такие оптимайзеры становятся некорректными. 
    

# API

[**FSDP API**](https://pytorch.org/docs/stable/fsdp.html)

```python
torch.distributed.fsdp.FullyShardedDataParallel(
	module,
	process_group=None,
	sharding_strategy=None,
	cpu_offload=None,
	auto_wrap_policy=None,
	backward_prefetch=BackwardPrefetch.BACKWARD_PRE,
	mixed_precision=None,
	ignored_modules=None,
	param_init_fn=None,
	device_id=None,
	sync_module_states=False,
	forward_prefetch=False,
	limit_all_gathers=True,
	use_orig_params=False,
	ignored_states=None,
	device_mesh=None,
)
```

- **auto_wrap_policy**
    
    `Optional[Union[Callable[[nn.Module, bool, int], bool], ModuleWrapPolicy, CustomPolicy]])`
    
    [ModuleWrapPolicy](https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/wrap.py#L185), в которую мы можем передать типы модулей, которые бы мы хотели обернуть в юниты (например. Супер полезно, например, для трансформеров, чтобы передать типа блока трансформера (например, LlamaDecoderLayer) и чтобы все блоки оборачивать целиком в один юнит. 
    
    [CustomPolicy](https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/wrap.py#L222), которая возвращает true, false или непустой, пустой словарь для nn.Module и либо оборачивает, либо нет, с аргументами из словаря, если возвращаем непустой словарь.
    
    - code
        
        ```python
        model = init_transformer_model(...)
        
        def lambda_fn(module: nn.Module):
            if module is model.lm_head:
                return {"sharding_strategy": ShardingStrategy.SHARD_GRAD_OP}
            elif isinstance(module, TransformerBlock):
                return True
             return False
        
        policy = CustomPolicy(lambda_fn)
        fsdp_model = FSDP(model, auto_wrap_policy=policy)
        ```
        
    
    Или, это может быть функция с тремя обязательными аргументами:
    
    ```python
    def custom_auto_wrap_policy(
    		module: nn.Module,
    		recurse: bool,
    		nonwrapped_numel: int,
    		# Additional custom arguments
    		min_num_params: int = int(1e8),
    ) -> bool:
    		return nonwrapped_numel >= min_num_params
    
    ```
    
    FSDP делает обход в глубину и для каждого узла вызывает эту функцию, которая говорит нам, нужно ли обернуть его в юнит. `module` - текущий узел, `recurse` - говорит о том, спускаемся ли мы или поднимаемся мы сейчас (передается FSDP), `nonwrapped_numel` - количество не обернутых элементов (передается FSDP). В примере выше, написана size-wrapped policy. 
    
    Больше примеров тут:
    
    [https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/wrap.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/fsdp/wrap.py)
    
    - как делаем обход?
        
        ```python
        def __auto_wrap_policy(
        		module: nn.Module,
        		recurse: bool,
        		nonwrapped_numel: int,
        ) -> bool:
            print(f'recurse {recurse} \t {type(module)} \t {nonwrapped_numel}')
            return True
        
        model = Model().cuda(dist.get_local_rank())
        model = FSDP(model, auto_wrap_policy=__auto_wrap_policy)
        ```
        
        ![Снимок экрана 2024-06-07 в 14.28.13.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_14.28.13.png)
        
    
    Еще раз о том, почему может быть важно использовать оптимальную политику оборачивания модели:
    
    - модель 1
        
        ![Снимок экрана 2024-06-07 в 14.12.21.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_14.12.21.png)
        
        время обучения
        
        ![Снимок экрана 2024-06-07 в 14.12.59.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_14.12.59.png)
        
    - модель 2
        
        ![Снимок экрана 2024-06-07 в 14.16.43.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_14.16.43.png)
        
        и скорость
        
        ![Снимок экрана 2024-06-07 в 14.17.12.png](FSDP%20a89deb29431a4ff38a0fa4e555b2749d/%25D0%25A1%25D0%25BD%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BA_%25D1%258D%25D0%25BA%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B0_2024-06-07_%25D0%25B2_14.17.12.png)
        

- **param_init_fn**
    
    Мы можем инициализировать модель на meta девайсе без выделения физической памяти под нее, тогда **param_init_fn** должна сказать торчу, как нужно инициализировать тот или иной модуль при переходе на девайс. Если такой функции нет, то мы попробуем позвать метод **reset_parameters**.
    
    Примеры **param_init_fn** из llm-foundry.
    
    [https://github.com/mosaicml/llm-foundry/blob/main/llmfoundry/models/utils/param_init_fns.py](https://github.com/mosaicml/llm-foundry/blob/main/llmfoundry/models/utils/param_init_fns.py)
    
- **sync_module_states**
    
    Если `True` , то мы сделаем broadcast модулей с нулевого ранга на другие ранги перед началом обучения.
    

- **param_init_fn +** **sync_module_states = effective simple initialization**
    
    
    [https://github.com/pytorch/torchtune/blob/main/recipes/lora_finetune_distributed.py#L281](https://github.com/pytorch/torchtune/blob/main/recipes/lora_finetune_distributed.py#L281)
    
    1. инициализируем модель на cpu только для rank0, остальные ранги на meta девайсе;
    2. подгрузим веса через load_state_dict только на rank0;
    3. установим `sync_module_states=True`
    4. тогда при оборачивании модели в FSDP, мы подгрузим в CPU только одну копию модели вместо world_size и раскидаем ее на шарды по другим девайсам.
    
    От меня - для других девайсов нужно все равно указывать param_init_fn, поскольку это meta девайсы. Впрочем, можно опять же выбрать тупую инициализацию:
    
    ```python
    model = FSDP(
        module=model,
        auto_wrap_policy=utils.lora_fsdp_wrap_policy(
            modules_to_wrap={modules.TransformerDecoderLayer}
        ),
        sharding_strategy=torch.distributed.fsdp.ShardingStrategy.FULL_SHARD,
        device_id=self._device,
        # Ensure we broadcast params and buffers from rank 0
        sync_module_states=True,
        # Initialize empty modules on all non-zero ranks
        param_init_fn=(
            lambda module: module.to_empty(
                device=torch.device("cuda"), recurse=False
            )
            if not self._is_rank_zero
            else None
        ),
    )
    ```
    
    из обсуждение с pytorch forum про то же самое:
    
    [How to use FSDP with meta device for pre-trained models?](https://discuss.pytorch.org/t/how-to-use-fsdp-with-meta-device-for-pre-trained-models/194389)
    

- **cpu_offload**
    
    Если совсем все плохо с памятью, то можно настроить выгрузку разных частей модели на CPU.
    

- **ignored_modules and ignored_states**
    
    Модули или параметры, которые мы не должны разбивать на юниты. Полезно, поскольку можем исключить оборачивание маленьких слоев. 
    
- **use_orig_params**
    
    для peft, когда модель содержит обучаемые и необучаемые параметры; 
    
    [[FSDP] `use_orig_params=True`: allow non-uniform `requires_grad` during init · Issue #91167 · pytorch/pytorch](https://github.com/pytorch/pytorch/issues/91167#issuecomment-1598664499)